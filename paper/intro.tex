\section{Introduction}
\label{sec:-intro}

Education sometimes seems like a process of ever-increasing 
specialization. Preschool acclimatizes larval humans to basic
scholarly etiquette (listening, sharing, naptime) grade-school begins 
to separate learning into a number of different subjects, and by the 
time college rolls around one is expected to pick a major.  By the time 
one has reached the PhD program, one's course of study has narrowed 
down to a small tunnel of the unknown, sometimes just a single problem.

When dealing with questions on the frontiers of science, it is not 
uncommon to encounter a problem that falls within the purview of many
such narrow fields, necessitating the addition of more and more team 
members.  This is something a problem, because, as any software engineer 
can tell you\cite{mythical}, throwing more people at a problem is not 
exactly a solution.  Despite how interested our University 
Administration seems in the subject, interdisciplinary research 
just doesn't seem to scale very well.

One of the real draws for machine learning lies in allowing us to 
compartmentalize our need for understanding, avoiding issues by 
transforming our problems into a familiar domain and extracting 
the answers we need.  So long as our model ``understands'' a problem, 
we can ignore a certain level of complexity and move on to the 
solving the interest parts of the problem.  This allows a computer 
scientist with an aversion to the so-called ``outdoors'' to make a 
number of correct judgments about forests, all from the comfort of 
their office.

From our limited experiences in the outdoors, we can make a few judgments about
the construct of a forest given a few data points. An accomplished expert 
in the fields of ecology or geography, specializing in the climate of 
that forest, might be able to tell us quite a bit more information based 
on that same starting data.  In the noble tradition of computer science, we 
would like to avoid doing that much work, and we will apply machine 
learning techniques to accomplish this task.  To that end, we consider 
the following multiclass classification problem. We construct a 
classifier that, when presented with a number of cartographic variables 
that apply to a 30--30 meter cell of forest, classifies such a cell by 
the predominant type of tree cover found therein.

In this paper, we utilize a multi-level ensemble method of 
classification.  We add a number of new features, scale the feature 
vector appropriately, and then train a collection of different 
classifiers on that data.  The classifiers vote to determine the 
classification of an example, and in some cases, pass on that vote into 
another layer of machines trained on a smaller subset of the training 
data.  We accomplish these tasks using Python and a series of libraries 
designed for data analysis and machine learning.  In so doing, our model 
reaches approximately 80\% accuracy on the Kaggle data set. 






\begin{table}
  \begin{tabular}{ l | r }
    \hline
    Elevation & Elevation in meters \\
    \hline
    Aspect & Aspect in degrees azimuth \\
    \hline
    Slope & Slope in degrees \\
    \hline
    Horizontal\_Distance\_To\_Hydrology & Meters to nearest water-body \\
    \hline
    Vertical\_Distance\_To\_Hydrology & Meters to nearest water-body \\
    \hline
    Horizontal\_Distance\_To\_Roadways & Meters to nearest roadway \\
    \hline
    Hillshade\_9am (0 to 255 index) & Hillshade index at 9am \\
    \hline
    Hillshade\_Noon (0 to 255 index) & Hillshade index at noon \\
    \hline
    Hillshade\_3pm (0 to 255 index) & Hillshade index at 3pm \\
    \hline
    Horizontal\_Distance\_To\_Fire\_Points & Meters, wildfire ignition points \\
    \hline
    Wilderness\_Area & Wilderness area designation \\
    \hline
    Soil\_Type & Soil Type designation \\
    \hline
  \end{tabular}
  \caption{Feature List}
  \label{table:featurelist}
\end{table}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
