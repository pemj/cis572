\section{Introduction}
\label{sec:-intro}

It is not uncommon to model education as a process of ever-increasing 
specialization. In grade school we are taught about a wide survey of
subjects from geography to history and reading, but as we get into high 
school and college we are encouraged to focus on one or two areas of 
study.  By the time one has reached the PhD program, one's course of 
study has narrowed down to a mere fraction of the former breadth of 
research, sometimes to a single problem.

One of the real draws for machine learning lies in allowing us to 
compartmentalize our need for understanding. In a similar manner to the increase in
focus of schooling, machine learning allows us to condense a breadth of problems
in different fields into a format that can be understood by a single algorithm.
So long as our model 
``understands'' a problem, we can ignore a certain level of complexity 
and move on to the interesting part of extracting useful features to
produce an accurate solution.  This lets a computer scientist 
with an aversion to engaging in or even thinking too deeply about the 
outdoors make a number of correct judgments about forests, 
all from the comfort of their office.

From our limited experiences in the outdoors, we can make a few judgments about
the construct of a forest given a few data points. An accomplished expert 
in the fields of ecology or geography, specializing in the climate of 
that forest, might be able to tell us quite a bit more information based 
on that same starting data.  In the noble tradition of computer science, we 
would like to avoid doing that much work, and we will apply machine 
learning techniques to accomplish this task.  To that end, we consider 
the following multiclass classification problem. We construct a 
classifier that, when presented with a number of cartographic variables 
that apply to a 30--30 meter cell of forest, classifies such a cell by 
the predominant type of tree cover found therein.

In this paper, we utilize a multi-level ensemble method of 
classification.  We add a number of new features, scale the feature 
vector appropriately, and then train a collection of different 
classifiers on that data.  The classifiers vote to determine the 
classification of an example, and in some cases, pass on that vote into 
another layer of machines trained on a smaller subset of the training 
data.  We accomplish these tasks using Python and a series of libraries 
designed for data analysis and machine learning.  In so doing, our model 
reaches approximately 80\% accuracy on the Kaggle data set. 






\begin{table}
  \begin{tabular}{ l | r }
    \hline
    Elevation & Elevation in meters \\
    \hline
    Aspect & Aspect in degrees azimuth \\
    \hline
    Slope & Slope in degrees \\
    \hline
    Horizontal\_Distance\_To\_Hydrology & Meters to nearest water-body \\
    \hline
    Vertical\_Distance\_To\_Hydrology & Meters to nearest water-body \\
    \hline
    Horizontal\_Distance\_To\_Roadways & Meters to nearest roadway \\
    \hline
    Hillshade\_9am (0 to 255 index) & Hillshade index at 9am \\
    \hline
    Hillshade\_Noon (0 to 255 index) & Hillshade index at noon \\
    \hline
    Hillshade\_3pm (0 to 255 index) & Hillshade index at 3pm \\
    \hline
    Horizontal\_Distance\_To\_Fire\_Points & Meters, wildfire ignition points \\
    \hline
    Wilderness\_Area & Wilderness area designation \\
    \hline
    Soil\_Type & Soil Type designation \\
    \hline
  \end{tabular}
  \caption{Feature List}
  \label{table:featurelist}
\end{table}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
