\section{Results}
\label{sec:-res}
The first classifier we attempted was Random Forest, primarily out of an 
appreciation for the pun value of using a forest to classify forests. 
Initially, our accuracy hovered between 60\% and 70\% on our validation 
data.  When we tuned the hyperparameters (increasing the number of 
features considered when splitting a tree, adding more trees to the 
forest, etc) of the Random Forest model, we reached 80\% accuracy on 
our validation set.  Unfortunately, our accuracy on the Kaggle 
competition remained below 70\%.  

We undertook similar experiments with the Gradient Boosting classifier 
(optimal results with an increased learning rate of approximately one 
third, increased max depth of five, increasing the number of boosting 
stages to 400, examining all features for a split), with somewhat 
better results (peak accuracy of 85\% on the validation set).  

We found that a k-value of 5 and a Euclidean distance measure
produced the highest-quality classifier from the k-Nearest Neighbor, 
model.  With those values, we reached a validation accuracy of about 
82\%.

Using an rbf kernel and a surprisingly high penalty parameter, with 
arbitrarily many iterations, our SVM reached approximately 81\% 
accuracy.

We attempted a Naive Bayes model, but were unable to reach an accuracy 
greater than 50\%.  We suspect that this low accuracy is a result of 
the interconnectedness of our feature vector.  As Naive Bayes relies on 
an assumption of independence between variables within the feature 
vector, and nature is somewhat infamously non-independent\cite{silent}, 
we are unsurprised by this result.

We carried out these tuning operations by way of an iterative binary 
search.  One hyperparameter at a time, we preceded to run the 
classifier with converging values until we had reached a sufficiently 
small difference in accuracy.  We recorded these values and explored 
the impact that small changes in multiple hyperparameters had on the 
accuracy of the model.  




From the first attempt at a Random Tree classifier, up to our final, 
somewhat convoluted approach to classification, we observed accuracy 
levels between
We'll want to talk about final accuracy, about the effects of various changes 
we made to the model on accuracy, about  

Interestingly, some of the more puzzling and impenetrable features 
were revealed to be the most important across all of the classifiers.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
