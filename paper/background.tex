\section{Background}
\label{sec:-back}
\subsection{Problem Structure}
The forest coverage classification problem clearly fits into the 
category of a multiclass classification problem, in which we describe 
a classifier that maps from a feature vector to one of a series of 
possible output labels.  This feature vector, as outlined in table 
\ref{table:featurelist}, contains a mix of continuous and discrete 
values.  The discrete features, as visualized in figures \ref{fig:soil} 
and \ref{fig:wilderness}, account for multiclass features encoded with 
the one-hot encoding method.  

Given these features, we wish to classify our example with a choice 
from the labels 
below:
\begin{enumerate}
\item Spruce/Fir
\item Lodgepole Pine
\item Ponderosa Pine
\item Cottonwood/Willow
\item Aspen
\item Douglas-fir
\item Krummholz
\end{enumerate}

Unlike a binary classifier, it is quite easy to end up with an 
error rate well over 50\% when dealing with a multiclass 
classifier.  As we're dealing data collected from a natural source, we 
expect we will find a high degree of noise within the data set.


\begin{figure*}
\centering
\includegraphics[width=\linewidth]{continuous}
 \caption{Continuous variables}
 \label{fig:continuous_features}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{soil_type}
 \caption{Soil types and accompanying cover types}
 \label{fig:soil}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{wilderness}
 \caption{Wilderness types and accompanying cover types}
 \label{fig:wilderness}
\end{figure*}

\subsection{Solution}
The classification solution we assemble to solve this problem involves 
a number of different machine learning models.  We began the project by 
attempting a Random Forest classifier, but eventually expanded to also 
make use of a Gradient Boost Machine, a Support Vector Machine, and a 
k-Nearest Neighbor classifier.  Each of these machines has its own 
strengths and weaknesses, which we will discuss as follows.  Each model 
has a number of hyperparameters that must be tuned properly before 
evaluating the total efficacy of such a technique: we will cover the 
tuning process in the following sections.

A Random Forest classifier\cite{breiman2001random} is an example of an 
ensemble method, a sort of mashup of decision trees with a machine 
learning technique called bootstrap aggregating, or, more colloquially, 
bagging\cite{bagging}.  Instead of constructing a single tree on the 
entirety of the training set, Random Forest constructs many smaller 
trees on randomly selected subsets of the training data.  This has a 
number of perks: Random Forests are resistant to artifacts of training 
set ordering and are highly robust in the face of noise and superfluous 
features.  Perhaps most importantly, Random Forests, by effectively 
averaging together a number of decision trees, can effectively mitigate 
the tendency of decision trees to overfit the training data.  As a 
small bonus, the structure of the algorithm allows for efficient 
parallelism, making this a very quick algorithm on a computer with 
sufficient cores.

When we began exploring the idea of adding further classifiers, we 
settled on the Grading Boosting Machine\cite{gbm} after some small 
amount of testing.  Like Random Forest, GBM is an ensemble method that 
constructs a number of subsidiary decision trees.  In this case, the 
algorithm combines decision trees with the idea of 
boosting\cite{boosting}.  Boosting is, in abstract, the process of 
combining a number of poor predictors into a single good predictor.  
In the case of a tree-based GBM, our process is superficially similar 
to bagging, insofar as we create trees based on repeatedly selecting 
subsets from the total training set.  The difference lies primarily in 
how we weight such trees.  Instead of relying largely on the sheer mass 
of trees, we instead attempt to select subtrees that will improve 
currently weak areas of the aggregate predictor.

k-Nearest Neighbor\cite{nearest} is arguably the simplest algorithm we 
deploy in the course of this paper.  We consider an n-dimensional 
graph (for a total of n features) over the feature space.  By populating 
the graph with our training examples, our predictor takes the form of 
simply checking the class of the k training examples that are closest 
to the testing instance.  This distance metric can be a simple 
Euclidean distance calculation, or one of a number of more complex 
metrics suited for particular problem domains.

Support Vector Machines


Most of the algorithms we made use of for this classification problem 
were provided by scikit-learn, with a couple of important exceptions.  
When mixing our classifiers together into a voting body, we rolled our 
own method of voting.

In addition, the confusion matrix as detailed in figure 
\ref{fig:confusion} motivated a second layer of classifier, where 
certain classes of classifications cause our system to double-check its 
work.  


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
