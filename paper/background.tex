\section{Background}
\label{sec:-back}
\subsection{Problem Structure}
The forest coverage classification problem clearly fits into the 
category of a multiclass classification problem, in which we describe 
a classifier that maps from a feature vector to one of a series of 
possible output labels.  This feature vector, as outlined in table 
\ref{table:featurelist}, contains a mix of continuous and discrete 
values.  The discrete features, as visualized in figures \ref{fig:soil} 
and \ref{fig:wilderness}, account for multiclass features encoded with 
the one-hot encoding method.  

Given these features, we wish to classify our example with a choice 
from the labels 
below:
\begin{enumerate}
\item Spruce/Fir
\item Lodgepole Pine
\item Ponderosa Pine
\item Cottonwood/Willow
\item Aspen
\item Douglas-fir
\item Krummholz
\end{enumerate}

Unlike a binary classifier, it is quite easy to end up with an 
error rate well over 50\% when dealing with a multiclass 
classifier.


\begin{figure*}
\centering
\includegraphics[width=\linewidth]{continuous}
 \caption{Continuous variables}
 \label{fig:continuous_features}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{soil_type}
 \caption{Soil types and accompanying cover types}
 \label{fig:soil}
\end{figure*}

\begin{figure*}
\centering
\includegraphics[width=\linewidth]{wilderness}
 \caption{Wilderness types and accompanying cover types}
 \label{fig:wilderness}
\end{figure*}

\subsection{Solution}
The classification solution we assemble to solve this problem involves 
a number of different machine learning models.  We began the project by 
attempting a Random Forest classifier, but eventually expanded to also 
make use of a Gradient Boost Machine, a Support Vector Machine, and a 
k-Nearest Neighbor classifier.  Each of these machines has its own 
strengths and weaknesses, which we will discuss as follows.  Each model 
has a number of hyperparameters that must be tuned properly before 
evaluating the total efficacy of such a technique: we will cover the 
tuning process in the following sections.

A Random Forest classifier\cite{breiman2001random} is an example of an 
ensemble method, a sort of mashup of decision trees with a machine 
learning technique called bootstrap aggregating, or, more colloquially, 
bagging\cite{bagging}.  Instead of constructing a single tree on the 
entirety of the training set, Random Forest constructs many smaller 
trees on randomly selected subsets of the training data.  This has a 
number of perks: Random Forests are resistant to artifacts of training 
set ordering and are highly robust in the face of noise and superfluous 
features.  Perhaps most importantly, Random Forests, by effectively 
averaging together a number of decision trees, can effectively mitigate 
the tendency of decision trees to overfit.

Most of the algorithms we made use of for this classification problem 
were provided by scikit-learn, with a couple of important exceptions.  
When mixing our classifiers together into a voting body, we rolled our 
own method of voting.

In addition, the confusion matrix as detailed in figure 
\ref{fig:confusion} motivated a second layer of classifier, where 
certain classes of classifications cause our system to double-check its 
work.  


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
