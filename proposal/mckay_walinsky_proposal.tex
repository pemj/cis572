\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\author{
  Peter McKay\\
  \texttt{pem@cs.uoregon.edu}
  \and
  Daniel Walinsky\\
  \texttt{dwalins4@uoregon.edu}
}
\title{Project Proposal}


\begin{document}

\maketitle

For our final project, we intend to tackle the following Kaggle problem:
the 
\href{http://www.kaggle.com/c/forest-cover-type-prediction}{Forest Cover Type Prediction}.
problem. In brief, we wish to provide a function that takes as input 
a collection of geographic and cartographic variables that apply to a 
given $30 x 30$ meter chunk of forest, and determines what kind of tree 
coverage will be most prevalent in that region.

Examples of data fields we have access to include the soil type 
(Cryaquolis, Granile, Rock outcrop, etc.), the elevation, and average slope.
Some inputs are boolean while others are linear, hinting that the use
of an ensemble of machine learning algorithms could be effective.

We are faced, then, with a multi-class classification problem. From the input 
data fields in our testing file, we wish to discover whether a 
particular cell can be categorized as
Spruce/Fir,
Lodgepole Pine,
Ponderosa Pine,
Cottonwood/Willow,
Aspen,
Douglas-fir, or
Krummholz.
The output of any machine-learning algorithm we come up with will 
therefore take the form of an integer from 1 to 7, inclusive.

Our project timeline, at a high level, is as follows:
\begin{enumerate}
\item Explore data, discover correlations, store hypotheses for later 
  use
\item Test different machine learning algorithms based on the previous 
  steps.
\item Settle on an approach (or combination of approaches) that return 
  a maximally effective result based on the a comparison to the testing 
  data.
\item Draw conclusions about the nature of our data, and problem as a 
  whole, based on the data from all earlier steps.
\end{enumerate}

As we proceed through this process, we intend to maintain notes at each 
stage, in order to parallelize, as much as possible, the experimentation 
and paper writing processes. We intend to focus on separate approaches 
to the problem, comparing and combining them as a final step.

Our first step must be to acquire the data and examine it, in order to 
discover what sort of model would best suit our data.  To that end, we 
must first settle upon a set of tools to assist us in this matter.  
We plan to carry out most of our research using Python.  More 
specifically, Python 3.4.2, with the help of a number of libraries 
designed to assist in machine learning and data analysis.  

\textbf{pandas: Python Data Analysis Library:}
We will make use of data structures and input mechanisms introduced in 
pandas.  These data structures, while containing a great deal of 
complex functionality, extend iterables with sufficient cleanliness to 
allow for compatibility with most other python libraries.

\textbf{matplotlib:} 
The first of these such libraries that sounds helpful is matplotlib.  
matplotlib extends python with some matlab-like functionality, allowing 
us to generate graphs and examine the data in question.  Should we need 
to scale the data, or remove a number of corrupted entries, matplotlib 
should give us an idea of where to start.

\textbf{numpy:}
numpy provides a collection of very useful tools for analysis, which 
will come in handy as we examine the variables, and any correlations 
they may have with one another.  numpy and matplotlib play quite nice 
together, and this will allow us to plot a number of interesting 
functions over the space of the data, testing hypotheses before we get 
to the proper machine learning part of the assignment.

\textbf{scikit-learn:}
In an effort to avoid duplicating the wheel (and running out of RAM 
while we try to get that wheel to load), we will be making use of 
scikit-learn for efficient implementations of the canonical algorithms 
as we test their efficacy.

While it may be poetically appropriate to make use of, say, an ensemble
of random forests, decision trees, and nearest-neighbor by way of
cover-tree, making such a decision at this stages in our project seems 
hasty.
More important than running through each of the built-in options from 
scikit-learn, however, is an effective choice of hyper-parameters for 
such options.  While any discussion of the specifics of these 
hyper-parameters would be premature at this juncture, we intend to 
inform such decisions with the data we gather in the earlier stages 
of the project.

\end{document}
